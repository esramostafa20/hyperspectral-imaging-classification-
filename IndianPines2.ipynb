{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpalJcNy9kkB"
      },
      "source": [
        "import keras,os\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation,AveragePooling2D\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXOyreDZ9kkD"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(ZeroPadding2D((1,1),input_shape=(4,4, 1)))\n",
        "model.add(Convolution2D(64, (3, 3), strides=(1,1), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(64, (3, 3), strides=(1,1), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2), strides=(1,1)))\n",
        " \n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(128, (3, 3), strides=(1,1), activation='relu'))\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "model.add(Convolution2D(128, (3, 3), strides=(1,1), activation='relu'))\n",
        "model.add(MaxPooling2D((1,1), strides=(1,1)))\n",
        "\n",
        "model.add(AveragePooling2D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(17,activation='softmax',kernel_initializer='he_normal'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nh3sRpHiNFg",
        "outputId": "418aec1d-7d98-44c3-9710-4515e545fbdf"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "zero_padding2d_9 (ZeroPaddin (None, 6, 6, 1)           0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 4, 4, 64)          640       \n",
            "_________________________________________________________________\n",
            "zero_padding2d_10 (ZeroPaddi (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 4, 4, 64)          36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 3, 3, 64)          0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_11 (ZeroPaddi (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 3, 3, 128)         73856     \n",
            "_________________________________________________________________\n",
            "zero_padding2d_12 (ZeroPaddi (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 3, 3, 128)         147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_3 (Average (None, 1, 1, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 17)                2193      \n",
            "=================================================================\n",
            "Total params: 261,201\n",
            "Trainable params: 261,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz6zyfXp9kkO"
      },
      "source": [
        "sgd = keras.optimizers.SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy'\n",
        "                  , optimizer = sgd\n",
        "                  , metrics=['accuracy']\n",
        "                 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnmcd4Qf9kkU"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "opt = Adam(lr=0.001)\n",
        "model.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S5f0D_lb9kkY",
        "scrolled": true,
        "outputId": "da6535ab-c6b5-4db5-e0f7-d539578d10e2"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "checkpoint = ModelCheckpoint(\"IndianPines2.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "early = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, mode='auto')\n",
        "model.fit(x_train, y_train, batch_size=512,validation_split=0.3, epochs=50,shuffle=True,callbacks=[checkpoint,early])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 14000 samples, validate on 6001 samples\n",
            "Epoch 1/50\n",
            "14000/14000 [==============================] - 7s 499us/step - loss: 75.9191 - accuracy: 0.3125 - val_loss: 3.1264 - val_accuracy: 0.5092\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.50925, saving model to IndianPines2.h5\n",
            "Epoch 2/50\n",
            "14000/14000 [==============================] - 6s 455us/step - loss: 1.8349 - accuracy: 0.5401 - val_loss: 1.3680 - val_accuracy: 0.5851\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.50925 to 0.58507, saving model to IndianPines2.h5\n",
            "Epoch 3/50\n",
            "14000/14000 [==============================] - 6s 455us/step - loss: 1.2269 - accuracy: 0.5962 - val_loss: 1.1344 - val_accuracy: 0.6096\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.58507 to 0.60957, saving model to IndianPines2.h5\n",
            "Epoch 4/50\n",
            "14000/14000 [==============================] - 6s 464us/step - loss: 1.1129 - accuracy: 0.6086 - val_loss: 1.0719 - val_accuracy: 0.6226\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.60957 to 0.62256, saving model to IndianPines2.h5\n",
            "Epoch 5/50\n",
            "14000/14000 [==============================] - 7s 466us/step - loss: 1.0657 - accuracy: 0.6219 - val_loss: 1.0780 - val_accuracy: 0.6237\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.62256 to 0.62373, saving model to IndianPines2.h5\n",
            "Epoch 6/50\n",
            "14000/14000 [==============================] - 7s 481us/step - loss: 1.0474 - accuracy: 0.6278 - val_loss: 1.0142 - val_accuracy: 0.6417\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.62373 to 0.64173, saving model to IndianPines2.h5\n",
            "Epoch 7/50\n",
            "14000/14000 [==============================] - 7s 478us/step - loss: 1.0213 - accuracy: 0.6354 - val_loss: 1.0668 - val_accuracy: 0.6259\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.64173\n",
            "Epoch 8/50\n",
            "14000/14000 [==============================] - 7s 479us/step - loss: 1.0137 - accuracy: 0.6340 - val_loss: 1.0318 - val_accuracy: 0.6164\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.64173\n",
            "Epoch 9/50\n",
            "14000/14000 [==============================] - 7s 481us/step - loss: 0.9941 - accuracy: 0.6384 - val_loss: 0.9774 - val_accuracy: 0.6602\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.64173 to 0.66022, saving model to IndianPines2.h5\n",
            "Epoch 10/50\n",
            "14000/14000 [==============================] - 7s 492us/step - loss: 0.9975 - accuracy: 0.6397 - val_loss: 0.9833 - val_accuracy: 0.6281\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.66022\n",
            "Epoch 11/50\n",
            "14000/14000 [==============================] - 7s 489us/step - loss: 0.9661 - accuracy: 0.6510 - val_loss: 0.9478 - val_accuracy: 0.6587\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.66022\n",
            "Epoch 12/50\n",
            "14000/14000 [==============================] - 7s 493us/step - loss: 0.9534 - accuracy: 0.6539 - val_loss: 0.9437 - val_accuracy: 0.6591\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.66022\n",
            "Epoch 13/50\n",
            "14000/14000 [==============================] - 7s 487us/step - loss: 0.9653 - accuracy: 0.6435 - val_loss: 0.9490 - val_accuracy: 0.6664\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.66022 to 0.66639, saving model to IndianPines2.h5\n",
            "Epoch 14/50\n",
            "14000/14000 [==============================] - 7s 498us/step - loss: 0.9410 - accuracy: 0.6547 - val_loss: 0.9353 - val_accuracy: 0.6566\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.66639\n",
            "Epoch 15/50\n",
            "14000/14000 [==============================] - 7s 474us/step - loss: 0.9321 - accuracy: 0.6566 - val_loss: 0.9304 - val_accuracy: 0.6654\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.66639\n",
            "Epoch 16/50\n",
            "14000/14000 [==============================] - 7s 474us/step - loss: 0.9386 - accuracy: 0.6579 - val_loss: 1.0232 - val_accuracy: 0.5997\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.66639\n",
            "Epoch 17/50\n",
            "14000/14000 [==============================] - 7s 468us/step - loss: 0.9457 - accuracy: 0.6524 - val_loss: 0.9207 - val_accuracy: 0.6494\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.66639\n",
            "Epoch 18/50\n",
            "14000/14000 [==============================] - 7s 477us/step - loss: 0.9095 - accuracy: 0.6646 - val_loss: 0.9177 - val_accuracy: 0.6589\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.66639\n",
            "Epoch 19/50\n",
            "14000/14000 [==============================] - 7s 475us/step - loss: 0.8975 - accuracy: 0.6712 - val_loss: 0.8692 - val_accuracy: 0.6821\n",
            "\n",
            "Epoch 00019: val_accuracy improved from 0.66639 to 0.68205, saving model to IndianPines2.h5\n",
            "Epoch 20/50\n",
            "14000/14000 [==============================] - 7s 469us/step - loss: 0.9188 - accuracy: 0.6607 - val_loss: 0.9992 - val_accuracy: 0.6609\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.68205\n",
            "Epoch 21/50\n",
            "14000/14000 [==============================] - 7s 468us/step - loss: 0.9360 - accuracy: 0.6536 - val_loss: 0.8794 - val_accuracy: 0.6751\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.68205\n",
            "Epoch 22/50\n",
            "14000/14000 [==============================] - 7s 488us/step - loss: 0.8950 - accuracy: 0.6655 - val_loss: 0.8914 - val_accuracy: 0.6542\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.68205\n",
            "Epoch 23/50\n",
            "14000/14000 [==============================] - 7s 482us/step - loss: 0.8783 - accuracy: 0.6746 - val_loss: 0.8932 - val_accuracy: 0.6629\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.68205\n",
            "Epoch 24/50\n",
            "14000/14000 [==============================] - 7s 476us/step - loss: 0.8801 - accuracy: 0.6753 - val_loss: 0.9963 - val_accuracy: 0.6296\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.68205\n",
            "Epoch 25/50\n",
            "14000/14000 [==============================] - 7s 485us/step - loss: 0.9105 - accuracy: 0.6628 - val_loss: 0.8576 - val_accuracy: 0.6822\n",
            "\n",
            "Epoch 00025: val_accuracy improved from 0.68205 to 0.68222, saving model to IndianPines2.h5\n",
            "Epoch 26/50\n",
            "14000/14000 [==============================] - 7s 479us/step - loss: 0.9153 - accuracy: 0.6617 - val_loss: 0.8620 - val_accuracy: 0.6837\n",
            "\n",
            "Epoch 00026: val_accuracy improved from 0.68222 to 0.68372, saving model to IndianPines2.h5\n",
            "Epoch 27/50\n",
            "14000/14000 [==============================] - 7s 475us/step - loss: 0.8742 - accuracy: 0.6738 - val_loss: 0.8748 - val_accuracy: 0.6761\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.68372\n",
            "Epoch 28/50\n",
            "14000/14000 [==============================] - 7s 481us/step - loss: 0.8691 - accuracy: 0.6807 - val_loss: 0.8565 - val_accuracy: 0.6789\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.68372\n",
            "Epoch 29/50\n",
            "14000/14000 [==============================] - 7s 481us/step - loss: 0.8557 - accuracy: 0.6806 - val_loss: 0.8630 - val_accuracy: 0.6804\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.68372\n",
            "Epoch 30/50\n",
            "14000/14000 [==============================] - 7s 476us/step - loss: 0.8581 - accuracy: 0.6776 - val_loss: 0.8557 - val_accuracy: 0.6789\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.68372\n",
            "Epoch 31/50\n",
            "14000/14000 [==============================] - 7s 476us/step - loss: 0.8854 - accuracy: 0.6713 - val_loss: 0.8633 - val_accuracy: 0.6749\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.68372\n",
            "Epoch 32/50\n",
            "14000/14000 [==============================] - 7s 474us/step - loss: 0.8694 - accuracy: 0.6769 - val_loss: 0.8330 - val_accuracy: 0.6854\n",
            "\n",
            "Epoch 00032: val_accuracy improved from 0.68372 to 0.68539, saving model to IndianPines2.h5\n",
            "Epoch 33/50\n",
            "14000/14000 [==============================] - 7s 480us/step - loss: 0.8400 - accuracy: 0.6886 - val_loss: 0.8220 - val_accuracy: 0.6984\n",
            "\n",
            "Epoch 00033: val_accuracy improved from 0.68539 to 0.69838, saving model to IndianPines2.h5\n",
            "Epoch 34/50\n",
            "14000/14000 [==============================] - 7s 477us/step - loss: 0.8365 - accuracy: 0.6874 - val_loss: 0.8453 - val_accuracy: 0.6797\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.69838\n",
            "Epoch 35/50\n",
            "14000/14000 [==============================] - 7s 473us/step - loss: 0.8760 - accuracy: 0.6694 - val_loss: 0.8966 - val_accuracy: 0.6527\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.69838\n",
            "Epoch 36/50\n",
            "14000/14000 [==============================] - 7s 473us/step - loss: 0.8383 - accuracy: 0.6831 - val_loss: 0.8407 - val_accuracy: 0.6747\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.69838\n",
            "Epoch 37/50\n",
            "14000/14000 [==============================] - 7s 473us/step - loss: 0.8424 - accuracy: 0.6853 - val_loss: 0.8410 - val_accuracy: 0.6867\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.69838\n",
            "Epoch 38/50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "14000/14000 [==============================] - 7s 477us/step - loss: 0.8324 - accuracy: 0.6876 - val_loss: 0.8844 - val_accuracy: 0.6754\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.69838\n",
            "Epoch 39/50\n",
            "14000/14000 [==============================] - 7s 473us/step - loss: 0.8337 - accuracy: 0.6891 - val_loss: 0.8230 - val_accuracy: 0.6972\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.69838\n",
            "Epoch 40/50\n",
            "14000/14000 [==============================] - 7s 476us/step - loss: 0.8362 - accuracy: 0.6870 - val_loss: 0.8144 - val_accuracy: 0.6954\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.69838\n",
            "Epoch 41/50\n",
            "14000/14000 [==============================] - 7s 473us/step - loss: 0.8381 - accuracy: 0.6860 - val_loss: 0.8072 - val_accuracy: 0.6956\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.69838\n",
            "Epoch 42/50\n",
            "14000/14000 [==============================] - 7s 479us/step - loss: 0.8281 - accuracy: 0.6877 - val_loss: 0.8019 - val_accuracy: 0.7027\n",
            "\n",
            "Epoch 00042: val_accuracy improved from 0.69838 to 0.70272, saving model to IndianPines2.h5\n",
            "Epoch 43/50\n",
            "14000/14000 [==============================] - 7s 474us/step - loss: 0.8095 - accuracy: 0.6943 - val_loss: 0.8511 - val_accuracy: 0.6779\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.70272\n",
            "Epoch 44/50\n",
            "14000/14000 [==============================] - 7s 476us/step - loss: 0.8110 - accuracy: 0.6956 - val_loss: 0.8827 - val_accuracy: 0.6581\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.70272\n",
            "Epoch 45/50\n",
            "14000/14000 [==============================] - 7s 475us/step - loss: 0.8397 - accuracy: 0.6875 - val_loss: 0.8133 - val_accuracy: 0.6922\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.70272\n",
            "Epoch 46/50\n",
            "14000/14000 [==============================] - 7s 477us/step - loss: 0.8166 - accuracy: 0.6967 - val_loss: 0.8562 - val_accuracy: 0.6782\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.70272\n",
            "Epoch 47/50\n",
            "14000/14000 [==============================] - 7s 475us/step - loss: 0.8280 - accuracy: 0.6917 - val_loss: 0.8624 - val_accuracy: 0.6827\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.70272\n",
            "Epoch 48/50\n",
            "14000/14000 [==============================] - 7s 476us/step - loss: 0.8239 - accuracy: 0.6879 - val_loss: 0.8189 - val_accuracy: 0.6999\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.70272\n",
            "Epoch 49/50\n",
            "14000/14000 [==============================] - 7s 474us/step - loss: 0.8287 - accuracy: 0.6884 - val_loss: 0.8756 - val_accuracy: 0.6686\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.70272\n",
            "Epoch 50/50\n",
            "14000/14000 [==============================] - 7s 475us/step - loss: 0.8118 - accuracy: 0.6901 - val_loss: 0.8135 - val_accuracy: 0.6967\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.70272\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fe0c1cd3c90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o18yBRPo-ADe"
      },
      "source": [
        "from scipy.io import loadmat\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA2V2ghpiNFi"
      },
      "source": [
        "x = loadmat('/content/drive/MyDrive/Downloads/Indian_pines.mat')\n",
        "Indian_pines = x['indian_pines']\n",
        "x = loadmat('/content/drive/MyDrive/Downloads/Indian_pines_corrected (1).mat')\n",
        "Indian_pines_corrected = x['indian_pines_corrected']\n",
        "x = loadmat('/content/drive/MyDrive/Downloads/Indian_pines_gt (1).mat')\n",
        "Indian_pines_gt = x['indian_pines_gt']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vpt73elziNFi",
        "outputId": "8af60f8a-bc32-4121-da01-1b926633f110"
      },
      "source": [
        "print(Indian_pines[1][0])\n",
        "print(len(Indian_pines[1][0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2576 4388 4334 4279 4543 4999 5026 4932 4953 4699 4669 4652 4526 4662\n",
            " 4694 4699 4629 4470 4398 4280 4302 4300 4226 4150 4189 4136 4081 4118\n",
            " 4098 3828 3985 3791 4007 4240 4191 4129 4738 5115 5004 3592 5048 5173\n",
            " 4960 4891 4559 4238 4462 4856 4665 4696 4630 4724 4648 3870 3745 3653\n",
            " 3285 2093 2250 2443 3105 2805 4010 4195 4142 4123 4095 4049 4000 3979\n",
            " 3852 3821 3723 3584 3229 2664 1706 1570 1811 1761 2179 2716 2843 2867\n",
            " 2905 2866 2957 3010 3088 3066 2961 2675 2764 2874 2638 2807 2818 2647\n",
            " 2418 2224 1827 1546 1093 1044 1012 1012 1031 1034 1050 1078 1106 1132\n",
            " 1237 1352 1254 1367 1620 1839 2000 2106 2170 2182 2178 2184 2065 2078\n",
            " 2112 2068 2051 2106 2102 2046 2032 2026 1988 1962 1938 1909 1848 1821\n",
            " 1764 1699 1678 1626 1529 1390 1287 1141 1040 1014 1015 1027 1008 1022\n",
            " 1007 1016 1016 1015 1002  993 1009 1014 1009 1032 1041 1103 1203 1290\n",
            " 1236 1087 1096 1204 1324 1314 1247 1248 1290 1362 1376 1390 1386 1386\n",
            " 1379 1367 1343 1337 1334 1323 1308 1292 1316 1309 1308 1283 1267 1268\n",
            " 1254 1255 1232 1230 1229 1209 1208 1180 1164 1157 1143 1128 1134 1123\n",
            " 1099 1096 1110 1086 1071 1077 1047 1030 1006 1015]\n",
            "220\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzhfLH87iNFj",
        "outputId": "edfba201-8ab3-490b-a5fc-139466494ef1"
      },
      "source": [
        "print(Indian_pines_gt[1][0])\n",
        "print(len(Indian_pines_gt))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "145\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMEsuJP2iNFj",
        "outputId": "e198ce32-1302-4714-f774-e511dba4250a"
      },
      "source": [
        "y=Indian_pines[i][j]\n",
        "x=[180,121,31,42,70,18,53,11,63,84,89,168,86,59,98]\n",
        "print(y[x])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1396 2205 3942 5058 3935 4549 3903 4769 4296 2977 3101 1244 3034 2480\n",
            " 2400]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COB2Vk40iNFk"
      },
      "source": [
        "training_data=[]\n",
        "i=0\n",
        "while(i<len(Indian_pines_gt)):\n",
        "    j=0\n",
        "    while(j<len(Indian_pines_gt[i])):\n",
        "        y=Indian_pines[i][j]\n",
        "        z=[180,121,31,42,70,18,53,11,63,84,89,168,86,59,98]\n",
        "        x=y[z]\n",
        "        x=np.array(x)\n",
        "        x=np.append(x,zero)\n",
        "        zero=np.array([0])\n",
        "        \n",
        "        x= np.array(x).reshape((4,4, 1 ))\n",
        "        training_data.append([x,Indian_pines_gt[i][j]])\n",
        "        j=j+1\n",
        "    \n",
        "    i=i+1\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dshjwvzdiNFk"
      },
      "source": [
        "import random\n",
        "random.shuffle(training_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ganlZw3HiNFk"
      },
      "source": [
        "x_train =[]\n",
        "y_train = []\n",
        "i=0\n",
        "for features,label in training_data:\n",
        "    if(i>20000):\n",
        "        break\n",
        "    x_train.append(features)\n",
        "    y_train.append(label)\n",
        "    i=i+1\n",
        "x_train=np.array(x_train)\n",
        "y_train=np.array(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QJGh_vGiNFk",
        "outputId": "5a3fcd23-6d86-4eb6-f0a4-fb64dfd4c88c"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20001, 4, 4, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJhDH0NfiNFl",
        "outputId": "2370018e-44ab-46d7-ec19-54c00706af3a"
      },
      "source": [
        "print(len(training_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZZLxjwSiNFl"
      },
      "source": [
        "x_val=x_train[20000:21025]\n",
        "y_val=y_train[20000:21025]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPnMrwhQiNFl",
        "outputId": "42b6565b-8608-421d-f3f1-c71058ddae83"
      },
      "source": [
        "x_val.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 4, 4, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ9wmFaoiNFm"
      },
      "source": [
        "# run this only once afte creating y_train\n",
        "y_train = keras.utils.to_categorical(y_train, 17)\n",
        "y_val=keras.utils.to_categorical(y_val, 17)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BioJzqBiNFm",
        "outputId": "d0697e14-d2c7-4a88-e8c9-fbf4351655a7"
      },
      "source": [
        "np.unique(y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8DwsAQGiNFm"
      },
      "source": [
        "from tensorflow.keras import models\n",
        "model= models.load_model(\"/content/drive/MyDrive/Downloads/Hyper.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMK3RdmRiNFm"
      },
      "source": [
        "from scipy.io import loadmat\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpivsRqmiNFn"
      },
      "source": [
        "x = loadmat('/content/drive/MyDrive/Downloads/Indian_pines.mat')\n",
        "Indian_pines = x['indian_pines']\n",
        "x = loadmat('/content/drive/MyDrive/Downloads/Indian_pines_corrected (1).mat')\n",
        "Indian_pines_corrected = x['indian_pines_corrected']\n",
        "x = loadmat('/content/drive/MyDrive/Downloads/Indian_pines_gt (1).mat')\n",
        "Indian_pines_gt = x['indian_pines_gt']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN98sMdniNFn"
      },
      "source": [
        "training_data=[]\n",
        "i=0\n",
        "while(i<len(Indian_pines_gt)):\n",
        "    j=0\n",
        "    while(j<len(Indian_pines_gt[i])):\n",
        "        x=Indian_pines[i][j]\n",
        "        x=np.array(x)\n",
        "        zero=np.array([0])\n",
        "        z=0\n",
        "        while(z<32):\n",
        "            x=np.append(x,zero)\n",
        "            z=z+1\n",
        "        x= np.array(x).reshape((16,16, 1 ))\n",
        "        training_data.append([x,Indian_pines_gt[i][j]])\n",
        "        j=j+1\n",
        "    i=i+1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrDHorFmiNFn"
      },
      "source": [
        "import random\n",
        "random.shuffle(training_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkKEWHHGiNFn"
      },
      "source": [
        "a = len(training_data)/8\n",
        "print(a)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srJJdY-MiNFn"
      },
      "source": [
        "x_train =[]\n",
        "y_train = []\n",
        "\n",
        "for features,label in training_data:\n",
        "    x_train.append(features)\n",
        "    y_train.append(label)\n",
        "x_train=np.array(x_train)\n",
        "y_train=np.array(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8pf7DWGiNFo"
      },
      "source": [
        "import tensorflow as tf\n",
        "samplenum=75655\n",
        "test=x_train[samplenum]\n",
        "test= np.array(test).reshape((1,16,16, 1 ))\n",
        "test = tf.cast(test, tf.float32)\n",
        "prediction=model.predict([test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XPWgsVjiNFo"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "calsses = ['Brocoli_green_weeds_1', 'Brocoli_green_weeds_2', 'Fallow', 'Fallow_rough_plow', 'Fallow_smooth', 'Stubble', 'Celery', 'Grapes_untrained', 'Soil_vinyard_develop', 'Corn_senesced_green_weeds', 'Lettuce_romaine_4wk', 'Lettuce_romaine_5wk', 'Lettuce_romaine_6wk', 'Lettuce_romaine_7wk', 'Vinyard_untrained', 'Vinyard_vertical_trellis', 'unknown']\n",
        "y_pos = np.arange(17)\n",
        "plt.bar(y_pos, prediction[0], alpha=1)\n",
        "result = np.where(prediction[0] == np.amax(prediction[0]))\n",
        "plt.ylabel('percentage')\n",
        "plt.title('class')\n",
        "plt.show()\n",
        "print('predicted class: ',calsses[result[0][0]])\n",
        "print ('Actual class: ',calsses[y_train[samplenum]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyUVKTACiNFo"
      },
      "source": [
        "from tensorflow.keras import models\n",
        "model= models.load_model(\"Hyper2.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A7JWr3wiNFp",
        "outputId": "62516f5b-874a-4894-94ab-962c6d6791e2"
      },
      "source": [
        "score = model.evaluate(x_val, y_val, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 1.0828599388715698\n",
            "Test accuracy: 0.66341466\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yhW5XdKiNFp"
      },
      "source": [
        "x_val= np.array(x_val).reshape((-1,16,16, 1 ))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ-kfkKHiNFp",
        "outputId": "08b96130-3fb9-4db6-a4d8-cc9098185eb9"
      },
      "source": [
        "print(len(x_val))\n",
        "print(len(y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1025\n",
            "1025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW4NqrNEiNFq"
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras \n",
        "import tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from tensorflow.keras.layers import AveragePooling2D, Input, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from skimage.feature import local_binary_pattern\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWXF92kriNFq"
      },
      "source": [
        "# Training parameters\n",
        "BATCH_SIZE = 32  # orig paper trained all networks with batch_size=128\n",
        "EPOCHS = 50 # 200\n",
        "USE_AUGMENTATION = True\n",
        "DEPTH = 3 * 6 + 2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wn5hBd0iNFq"
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 160:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 120:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WayspWvEiNFq"
      },
      "source": [
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLzBaqkZiNFr"
      },
      "source": [
        "def resnet_v1(input_shape, depth, num_classes=17):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "    ResNet32 0.46M\n",
        "    ResNet44 0.66M\n",
        "    ResNet56 0.85M\n",
        "    ResNet110 1.7M\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = tensorflow.keras.layers.add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "#     x = AveragePooling2D()(x)\n",
        "    flatten = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(flatten)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EppnfiLiNFs"
      },
      "source": [
        "# Create the neural network\n",
        "model = resnet_v1(input_shape=(4,4,1), depth=DEPTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "5mYzrnBriNFs",
        "outputId": "f444a946-eb53-4f2a-fe7f-e326544071ac"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 4, 4, 1)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 4, 4, 16)     160         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 4, 4, 16)     64          conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 4, 4, 16)     0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 4, 4, 16)     2320        activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 4, 4, 16)     64          conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 4, 4, 16)     0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 4, 4, 16)     2320        activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 4, 4, 16)     64          conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_27 (Add)                    (None, 4, 4, 16)     0           activation_57[0][0]              \n",
            "                                                                 batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 4, 4, 16)     0           add_27[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 4, 4, 16)     2320        activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 4, 4, 16)     64          conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 4, 4, 16)     0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 4, 4, 16)     2320        activation_60[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 4, 4, 16)     64          conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_28 (Add)                    (None, 4, 4, 16)     0           activation_59[0][0]              \n",
            "                                                                 batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 4, 4, 16)     0           add_28[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 4, 4, 16)     2320        activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 4, 4, 16)     64          conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 4, 4, 16)     0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 4, 4, 16)     2320        activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 4, 4, 16)     64          conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_29 (Add)                    (None, 4, 4, 16)     0           activation_61[0][0]              \n",
            "                                                                 batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 4, 4, 16)     0           add_29[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 2, 2, 32)     4640        activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 2, 2, 32)     128         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 2, 2, 32)     0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 2, 2, 32)     9248        activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 2, 2, 32)     544         activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 2, 2, 32)     128         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_30 (Add)                    (None, 2, 2, 32)     0           conv2d_72[0][0]                  \n",
            "                                                                 batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 2, 2, 32)     0           add_30[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 2, 2, 32)     9248        activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 2, 2, 32)     128         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 2, 2, 32)     0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 2, 2, 32)     9248        activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 2, 2, 32)     128         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_31 (Add)                    (None, 2, 2, 32)     0           activation_65[0][0]              \n",
            "                                                                 batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 2, 2, 32)     0           add_31[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 2, 2, 32)     9248        activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 2, 2, 32)     128         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 2, 2, 32)     0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 2, 2, 32)     9248        activation_68[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 2, 2, 32)     128         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_32 (Add)                    (None, 2, 2, 32)     0           activation_67[0][0]              \n",
            "                                                                 batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 2, 2, 32)     0           add_32[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 1, 1, 64)     18496       activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 1, 1, 64)     256         conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 1, 1, 64)     0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 1, 1, 64)     36928       activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 1, 1, 64)     2112        activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 1, 1, 64)     256         conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_33 (Add)                    (None, 1, 1, 64)     0           conv2d_79[0][0]                  \n",
            "                                                                 batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 1, 1, 64)     0           add_33[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 1, 1, 64)     36928       activation_71[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 1, 1, 64)     256         conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 1, 1, 64)     0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 1, 1, 64)     36928       activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 1, 1, 64)     256         conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_34 (Add)                    (None, 1, 1, 64)     0           activation_71[0][0]              \n",
            "                                                                 batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 1, 1, 64)     0           add_34[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 1, 1, 64)     36928       activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 1, 1, 64)     256         conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 1, 1, 64)     0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 1, 1, 64)     36928       activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 1, 1, 64)     256         conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_35 (Add)                    (None, 1, 1, 64)     0           activation_73[0][0]              \n",
            "                                                                 batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 1, 1, 64)     0           add_35[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 64)           0           activation_75[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 17)           1105        flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 274,609\n",
            "Trainable params: 273,233\n",
            "Non-trainable params: 1,376\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEtdTiTIiNFs",
        "outputId": "aede74c7-2168-49ec-bba5-a06f378c999d"
      },
      "source": [
        "sgd = keras.optimizers.SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy'\n",
        "                  , optimizer = sgd\n",
        "                  , metrics=['accuracy']\n",
        "                 )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "('Could not interpret optimizer identifier:', <keras.optimizers.SGD object at 0x7fe0e260d990>)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-de9bcf385de3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m model.compile(loss='categorical_crossentropy'\n\u001b[1;32m      4\u001b[0m                   \u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                   \u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                  )\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m         'experimental_run_tf_function', True)\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     is_any_optimizer_v1 = any(isinstance(opt, optimizers.Optimizer)\n\u001b[1;32m    253\u001b[0m                               for opt in nest.flatten(self.optimizer))\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_set_optimizer\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m   1452\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1454\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     if (self._dtype_policy.loss_scale is not None and\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizers.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Could not interpret optimizer identifier:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: ('Could not interpret optimizer identifier:', <keras.optimizers.SGD object at 0x7fe0e260d990>)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqFk-I0eiNFs"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "opt = Adam(lr=0.001)\n",
        "model.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "pNhwwvT9iNFt",
        "outputId": "9badf713-6e05-489a-9f93-63b7fafeaa3c"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "checkpoint = ModelCheckpoint(\"IndianPines2ResNet.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "early = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, mode='auto')\n",
        "model.fit(x_train, y_train, batch_size=512,validation_split=0.3, epochs=50,shuffle=True,callbacks=[checkpoint,early])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 14000 samples, validate on 6001 samples\n",
            "Epoch 1/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.6027 - accuracy: 0.7881\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.67072, saving model to IndianPines2ResNet.h5\n",
            "14000/14000 [==============================] - 3s 189us/sample - loss: 0.6051 - accuracy: 0.7871 - val_loss: 1.0485 - val_accuracy: 0.6707\n",
            "Epoch 2/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.6093 - accuracy: 0.7878\n",
            "Epoch 00002: val_accuracy improved from 0.67072 to 0.69788, saving model to IndianPines2ResNet.h5\n",
            "14000/14000 [==============================] - 3s 192us/sample - loss: 0.6096 - accuracy: 0.7876 - val_loss: 0.9271 - val_accuracy: 0.6979\n",
            "Epoch 3/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.6044 - accuracy: 0.7921\n",
            "Epoch 00003: val_accuracy did not improve from 0.69788\n",
            "14000/14000 [==============================] - 2s 178us/sample - loss: 0.6048 - accuracy: 0.7918 - val_loss: 1.2271 - val_accuracy: 0.6359\n",
            "Epoch 4/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5974 - accuracy: 0.7933\n",
            "Epoch 00004: val_accuracy did not improve from 0.69788\n",
            "14000/14000 [==============================] - 3s 188us/sample - loss: 0.5968 - accuracy: 0.7932 - val_loss: 1.3320 - val_accuracy: 0.6479\n",
            "Epoch 5/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5887 - accuracy: 0.7971\n",
            "Epoch 00005: val_accuracy did not improve from 0.69788\n",
            "14000/14000 [==============================] - 3s 193us/sample - loss: 0.5875 - accuracy: 0.7973 - val_loss: 1.0016 - val_accuracy: 0.6964\n",
            "Epoch 6/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5909 - accuracy: 0.7956\n",
            "Epoch 00006: val_accuracy did not improve from 0.69788\n",
            "14000/14000 [==============================] - 3s 192us/sample - loss: 0.5905 - accuracy: 0.7951 - val_loss: 0.9778 - val_accuracy: 0.6871\n",
            "Epoch 7/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.6041 - accuracy: 0.7896\n",
            "Epoch 00007: val_accuracy improved from 0.69788 to 0.69838, saving model to IndianPines2ResNet.h5\n",
            "14000/14000 [==============================] - 3s 212us/sample - loss: 0.6040 - accuracy: 0.7891 - val_loss: 1.0543 - val_accuracy: 0.6984\n",
            "Epoch 8/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5828 - accuracy: 0.7976\n",
            "Epoch 00008: val_accuracy did not improve from 0.69838\n",
            "14000/14000 [==============================] - 3s 198us/sample - loss: 0.5823 - accuracy: 0.7980 - val_loss: 1.1511 - val_accuracy: 0.6574\n",
            "Epoch 9/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5757 - accuracy: 0.8003\n",
            "Epoch 00009: val_accuracy did not improve from 0.69838\n",
            "14000/14000 [==============================] - 3s 196us/sample - loss: 0.5767 - accuracy: 0.8002 - val_loss: 1.2042 - val_accuracy: 0.6579\n",
            "Epoch 10/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5737 - accuracy: 0.7995\n",
            "Epoch 00010: val_accuracy improved from 0.69838 to 0.74138, saving model to IndianPines2ResNet.h5\n",
            "14000/14000 [==============================] - 3s 216us/sample - loss: 0.5754 - accuracy: 0.7991 - val_loss: 0.8159 - val_accuracy: 0.7414\n",
            "Epoch 11/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5849 - accuracy: 0.7979\n",
            "Epoch 00011: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 197us/sample - loss: 0.5862 - accuracy: 0.7976 - val_loss: 0.9906 - val_accuracy: 0.6741\n",
            "Epoch 12/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5837 - accuracy: 0.7963\n",
            "Epoch 00012: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 203us/sample - loss: 0.5829 - accuracy: 0.7965 - val_loss: 1.0068 - val_accuracy: 0.6876\n",
            "Epoch 13/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5812 - accuracy: 0.7967\n",
            "Epoch 00013: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 202us/sample - loss: 0.5810 - accuracy: 0.7969 - val_loss: 0.9357 - val_accuracy: 0.6996\n",
            "Epoch 14/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5634 - accuracy: 0.8053\n",
            "Epoch 00014: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 203us/sample - loss: 0.5640 - accuracy: 0.8054 - val_loss: 0.8800 - val_accuracy: 0.7227\n",
            "Epoch 15/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5677 - accuracy: 0.8011\n",
            "Epoch 00015: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 203us/sample - loss: 0.5672 - accuracy: 0.8016 - val_loss: 1.2872 - val_accuracy: 0.5887\n",
            "Epoch 16/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5588 - accuracy: 0.8051\n",
            "Epoch 00016: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 206us/sample - loss: 0.5582 - accuracy: 0.8057 - val_loss: 0.8568 - val_accuracy: 0.7274\n",
            "Epoch 17/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5567 - accuracy: 0.8064\n",
            "Epoch 00017: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 206us/sample - loss: 0.5586 - accuracy: 0.8053 - val_loss: 1.4121 - val_accuracy: 0.6196\n",
            "Epoch 18/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5698 - accuracy: 0.7996\n",
            "Epoch 00018: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 204us/sample - loss: 0.5701 - accuracy: 0.7996 - val_loss: 1.5235 - val_accuracy: 0.5641\n",
            "Epoch 19/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5728 - accuracy: 0.7983\n",
            "Epoch 00019: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 205us/sample - loss: 0.5729 - accuracy: 0.7984 - val_loss: 0.7928 - val_accuracy: 0.7400\n",
            "Epoch 20/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5520 - accuracy: 0.8079\n",
            "Epoch 00020: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 206us/sample - loss: 0.5528 - accuracy: 0.8079 - val_loss: 0.8713 - val_accuracy: 0.7237\n",
            "Epoch 21/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5545 - accuracy: 0.8074\n",
            "Epoch 00021: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 205us/sample - loss: 0.5535 - accuracy: 0.8078 - val_loss: 0.9535 - val_accuracy: 0.6919\n",
            "Epoch 22/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5517 - accuracy: 0.8094\n",
            "Epoch 00022: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 206us/sample - loss: 0.5526 - accuracy: 0.8089 - val_loss: 1.6159 - val_accuracy: 0.6014\n",
            "Epoch 23/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5451 - accuracy: 0.8094\n",
            "Epoch 00023: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 205us/sample - loss: 0.5460 - accuracy: 0.8091 - val_loss: 0.9808 - val_accuracy: 0.6821\n",
            "Epoch 24/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5583 - accuracy: 0.8055\n",
            "Epoch 00024: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 206us/sample - loss: 0.5592 - accuracy: 0.8050 - val_loss: 1.0581 - val_accuracy: 0.6802\n",
            "Epoch 25/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5491 - accuracy: 0.8070\n",
            "Epoch 00025: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 205us/sample - loss: 0.5509 - accuracy: 0.8064 - val_loss: 1.4823 - val_accuracy: 0.6184\n",
            "Epoch 26/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5452 - accuracy: 0.8098\n",
            "Epoch 00026: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 206us/sample - loss: 0.5444 - accuracy: 0.8103 - val_loss: 1.0800 - val_accuracy: 0.6849\n",
            "Epoch 27/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5291 - accuracy: 0.8186\n",
            "Epoch 00027: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 208us/sample - loss: 0.5288 - accuracy: 0.8184 - val_loss: 0.9882 - val_accuracy: 0.7154\n",
            "Epoch 28/50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5409 - accuracy: 0.8113\n",
            "Epoch 00028: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 206us/sample - loss: 0.5418 - accuracy: 0.8113 - val_loss: 0.9458 - val_accuracy: 0.7009\n",
            "Epoch 29/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5365 - accuracy: 0.8153\n",
            "Epoch 00029: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 205us/sample - loss: 0.5368 - accuracy: 0.8149 - val_loss: 1.6465 - val_accuracy: 0.5902\n",
            "Epoch 30/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5244 - accuracy: 0.8198\n",
            "Epoch 00030: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 207us/sample - loss: 0.5253 - accuracy: 0.8191 - val_loss: 1.2875 - val_accuracy: 0.6001\n",
            "Epoch 31/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5195 - accuracy: 0.8185\n",
            "Epoch 00031: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 208us/sample - loss: 0.5206 - accuracy: 0.8181 - val_loss: 0.8096 - val_accuracy: 0.7322\n",
            "Epoch 32/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5183 - accuracy: 0.8211\n",
            "Epoch 00032: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 208us/sample - loss: 0.5187 - accuracy: 0.8210 - val_loss: 0.8054 - val_accuracy: 0.7177\n",
            "Epoch 33/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5356 - accuracy: 0.8163\n",
            "Epoch 00033: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 209us/sample - loss: 0.5361 - accuracy: 0.8162 - val_loss: 1.4543 - val_accuracy: 0.5859\n",
            "Epoch 34/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5363 - accuracy: 0.8107\n",
            "Epoch 00034: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 213us/sample - loss: 0.5365 - accuracy: 0.8107 - val_loss: 0.8852 - val_accuracy: 0.7260\n",
            "Epoch 35/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5349 - accuracy: 0.8142\n",
            "Epoch 00035: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 208us/sample - loss: 0.5364 - accuracy: 0.8136 - val_loss: 0.9192 - val_accuracy: 0.7164\n",
            "Epoch 36/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5222 - accuracy: 0.8205\n",
            "Epoch 00036: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 213us/sample - loss: 0.5224 - accuracy: 0.8206 - val_loss: 1.0123 - val_accuracy: 0.7137\n",
            "Epoch 37/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5044 - accuracy: 0.8263\n",
            "Epoch 00037: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 212us/sample - loss: 0.5047 - accuracy: 0.8263 - val_loss: 0.9847 - val_accuracy: 0.6911\n",
            "Epoch 38/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5052 - accuracy: 0.8266\n",
            "Epoch 00038: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 210us/sample - loss: 0.5053 - accuracy: 0.8264 - val_loss: 0.9910 - val_accuracy: 0.7050\n",
            "Epoch 39/50\n",
            "13824/14000 [============================>.] - ETA: 0s - loss: 0.5106 - accuracy: 0.8220\n",
            "Epoch 00039: val_accuracy did not improve from 0.74138\n",
            "14000/14000 [==============================] - 3s 213us/sample - loss: 0.5111 - accuracy: 0.8216 - val_loss: 1.1709 - val_accuracy: 0.6541\n",
            "Epoch 00039: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe0b3eea3d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAgdKFEhiNFt"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}